# ðŸ† Comprehensive SWE Intelligence Benchmarking Configuration

execution:
  parallel_workers: 16
  timeout_minutes: 120
  retry_attempts: 3
  use_containers: true
  resource_limits:
    max_memory_gb: 32
    max_cpu_cores: 16

benchmarks:
  # ðŸŽ¯ SWE-Bench: Real-world GitHub issues (Primary Standard)
  swe_bench:
    enabled: true
    subset: "lite"  # Options: lite, full, all
    max_instances: 100
    timeout_per_instance: 300
    priority: "high"
    
  # ðŸ§‘â€ðŸ’» HumanEval & Variants: Programming problems
  humaneval:
    enabled: true
    variants: ["base", "plus", "mbpp"]
    max_problems: 164
    pass_k: [1, 5, 10, 25, 50, 100]
    num_samples: 100
    priority: "high"
    
  # ðŸŒ BigCode: Multi-language evaluation
  bigcode:
    enabled: true
    languages: ["python", "javascript", "java", "cpp", "go", "rust"]
    tasks: ["multipl_e", "ds_1000", "codexglue"]
    max_problems: 500
    priority: "medium"
    
  # ðŸ“š Repository-level Understanding
  repoeval:
    enabled: true
    max_repos: 50
    context_window: 16384
    tasks: ["completion", "bug_fixing", "feature_addition", "refactoring"]
    programming_languages: ["python", "javascript", "java"]
    priority: "medium"
    
  # ðŸ› ï¸ DevAI: Developer workflow simulation
  devai:
    enabled: true
    workflow_tasks: ["debugging", "testing", "refactoring", "code_review", "documentation"]
    max_scenarios: 100
    simulation_depth: "full"
    priority: "medium"
    
  # ðŸ”’ Security: Vulnerability detection and code quality
  security:
    enabled: true
    frameworks: ["cwe", "codeql", "semgrep", "bandit"]
    max_samples: 200
    vulnerability_types: ["injection", "xss", "auth", "crypto", "buffer_overflow"]
    priority: "low"
    
  # ðŸ“ Code Generation: Specialized tasks
  codegen:
    enabled: true
    tasks: ["conala", "codet5", "docstring", "comment_generation"]
    max_samples: 500
    metrics: ["bleu", "rouge", "meteor", "exact_match"]
    priority: "low"

# ðŸ“Š Scoring Configuration
scoring:
  # Benchmark weights for WPI calculation
  weights:
    swe_bench: 0.30
    humaneval: 0.20
    bigcode: 0.15
    repoeval: 0.15
    devai: 0.10
    security: 0.05
    codegen: 0.05
  
  # Confidence factors based on benchmark maturity
  confidence_factors:
    swe_bench: 1.0      # Mature, established
    humaneval: 1.0      # Mature, widely used
    humaneval_plus: 0.8 # Established variant
    mbpp: 0.8          # Established
    ds_1000: 0.8       # Established
    multipl_e: 0.8     # Established
    repoeval: 0.6      # Emerging
    devai_bench: 0.6   # Emerging
    cwe_bench: 0.6     # Emerging
    codeql: 0.8        # Established
    codet5: 0.8        # Established
    conala: 0.8        # Established
  
  # Performance thresholds
  thresholds:
    exceptional: 90    # SOTA level
    excellent: 80      # Production ready
    good: 70          # Competitive
    fair: 60          # Needs improvement
    poor: 0           # Significant gaps

# ðŸŽ¯ Target Baselines
targets:
  swe_bench_resolution_rate: 0.20      # >20% (current SOTA ~15%)
  humaneval_pass_at_1: 0.80           # >80% (current SOTA ~75%)
  multipl_e_average: 0.70             # >70% across languages
  ds_1000_completion: 0.60            # >60% completion rate
  repoeval_context_accuracy: 0.85     # >85% context accuracy
  security_false_positive_rate: 0.05  # <5% false positives

# ðŸ“ˆ Output Configuration
output:
  detailed_logs: true
  save_intermediate: true
  generate_report: true
  export_formats: ["json", "csv", "html"]
  
  # Dashboard configuration
  dashboard:
    enabled: true
    refresh_rate_seconds: 5
    port: 8080
    
  # Visualization options
  charts:
    enabled: true
    types: ["progress", "comparison", "trend", "category_breakdown"]
    
  # Notification settings
  notifications:
    enabled: false
    webhooks: []
    email: []

# ðŸ³ Docker Configuration
docker:
  enabled: true
  base_image: "python:3.11-slim"
  memory_limit: "8g"
  cpu_limit: "4"
  network_mode: "bridge"
  
  # Custom images for specific benchmarks
  custom_images:
    swe_bench: "swe_bench:latest"
    bigcode: "bigcode_eval:latest"
    security: "security_tools:latest"

# ðŸ”§ Tool Integration
tools:
  # AI models for evaluation
  models:
    primary: "claude-3-opus"
    fallback: "gpt-4-turbo"
    local: "codellama-34b"
    
  # External services
  services:
    github_api_key: null
    openai_api_key: null
    anthropic_api_key: null
    
  # Local tools
  local_tools:
    git: "/usr/bin/git"
    docker: "/usr/bin/docker"
    python: "/usr/bin/python3"

# âš¡ Performance Optimization
optimization:
  parallel_execution: true
  lazy_loading: true
  result_caching: true
  incremental_evaluation: true
  
  # Resource management
  memory_management:
    garbage_collection: true
    memory_threshold_gb: 24
    cleanup_interval_minutes: 30
    
  # Execution optimization
  batch_processing: true
  pipeline_stages: 4
  concurrent_benchmarks: 8

# ðŸ“‹ Quality Assurance
quality:
  # Validation rules
  validation:
    min_success_rate: 0.5
    max_error_rate: 0.1
    timeout_threshold: 0.05
    
  # Reproducibility
  reproducibility:
    random_seed: 42
    deterministic_execution: true
    version_tracking: true
    
  # Monitoring
  monitoring:
    performance_tracking: true
    resource_monitoring: true
    error_logging: true
    audit_trail: true